#   rÃ©cuperer C[i], crÃ©er le modÃ¨le, calculer les erreurs
#    ...
#  }
C = c(0.1,0.2,0.3,0.4,0.5,0.6)
for(i in 1:length(C)){
mod <- svm(yesno~.,data = train, kernel='linear',cost=C[i],type='C-classification', scale = T)
print("c = ")
print( C[i])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,7]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,7]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
# ----- Choix de C
### TODO: Essayer maintenant diffÃ©rentes valeurs de C (entre 0.001 et 100 par exemple), et
# calculer pour chacune de ces valeurs le pourcentage d'erreur en apprentissage
# et en validation
# Aide : Vous pouvez dÃ©finir un vecteur C (avec la commande c() ) contenant les diffÃ©rentes valeurs que vous voulez tester
# et faire une boucle sur les valeurs de C
# for(i in 1:length(C)){
#   rÃ©cuperer C[i], crÃ©er le modÃ¨le, calculer les erreurs
#    ...
#  }
C = c(0.7,0.8,0.9)
for(i in 1:length(C)){
mod <- svm(yesno~.,data = train, kernel='linear',cost=C[i],type='C-classification', scale = T)
print("c = ")
print( C[i])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,7]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,7]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
# ----- Choix de C
### TODO: Essayer maintenant diffÃ©rentes valeurs de C (entre 0.001 et 100 par exemple), et
# calculer pour chacune de ces valeurs le pourcentage d'erreur en apprentissage
# et en validation
# Aide : Vous pouvez dÃ©finir un vecteur C (avec la commande c() ) contenant les diffÃ©rentes valeurs que vous voulez tester
# et faire une boucle sur les valeurs de C
# for(i in 1:length(C)){
#   rÃ©cuperer C[i], crÃ©er le modÃ¨le, calculer les erreurs
#    ...
#  }
C = c(0.001,0.01,0.1,1,10,100)
for(i in 1:length(C)){
mod <- svm(yesno~.,data = train, kernel='linear',cost=C[i],type='C-classification', scale = T)
print("c = ")
print( C[i])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,7]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,7]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
library(e1071)
# ----- Chargement des fonctions utiles
source("./fonctions_utiles.R")
# ----- Chargement et analyse des donnÃ©es
dataset = read.table("./data/SepNonLineaire.txt", header = T)
View(dataset)
View(dataset)
View(dataset)
View(dataset)
View(dataset)
ntrain = floor(0.7 * nall) # number of rows for train: 70% (vous pouvez changer en fonction des besoins)
nvalid = floor(0.15 * nall) # number of rows for valid: 15% (idem)
ntest = nall - ntrain - nvalid # number of rows for test: le reste
nall = nrow(dataset) #total number of rows in data
ntrain = floor(0.7 * nall) # number of rows for train: 70% (vous pouvez changer en fonction des besoins)
nvalid = floor(0.15 * nall) # number of rows for valid: 15% (idem)
ntest = nall - ntrain - nvalid # number of rows for test: le reste
set.seed(20) # choix d une graine pour le tirage alÃ©atoire
index = sample(nall) # permutation alÃ©atoire des nombres 1, 2, 3 , ... nall
train = dataset[index[1:ntrain],] # crÃ©ation du jeu d'apprentissage
valid = dataset[index[(ntrain+1):(ntrain+nvalid)],] # crÃ©ation du jeu de validation
test = dataset[index[(ntrain+nvalid+1):(ntrain+nvalid+ntest)],] # crÃ©ation du jeu de test
nall = nrow(dataset) #total number of rows in data
ntrain = floor(0.7 * nall) # number of rows for train: 70% (vous pouvez changer en fonction des besoins)
nvalid = floor(0.15 * nall) # number of rows for valid: 15% (idem)
ntest = nall - ntrain - nvalid # number of rows for test: le reste
set.seed(20) # choix d une graine pour le tirage alÃ©atoire
index = sample(nall) # permutation alÃ©atoire des nombres 1, 2, 3 , ... nall
train = dataset[index[1:ntrain],] # crÃ©ation du jeu d'apprentissage
valid = dataset[index[(ntrain+1):(ntrain+nvalid)],] # crÃ©ation du jeu de validation
test = dataset[index[(ntrain+nvalid+1):(ntrain+nvalid+ntest)],] # crÃ©ation du jeu de test
View(test)
View(test)
View(valid)
View(valid)
View(valid)
View(train)
View(train)
# Affichage des donnÃ©es d'apprentissage et de validation (triangles)
plot(train[which(train$y==0),1:2], col="red", xlim = c(-3,4), ylim = c(-5,9))
points(train[which(train$y==1),1:2], col="blue")
points(valid[which(valid$y==0),1:2], col="red", pch = 2)
points(valid[which(valid$y==1),1:2], col="blue", pch = 2)
# ----- Apprentissage d'un SVM linÃ©aire (pour essayer quand meme)
# On essaye C = 1000 pour dÃ©buter
### TODO: Apprendre un modÃ¨le lineaire et calculer erreurs apprentissage/validation
svm_model <- svm(y~.,data = dataset, kernel='linear',cost=1000,type='C-classification', scale = F)
summary(svm_hard_cost2)
dessiner_marge(dataset, svm_hard_cost2, 0,20,0,20.5, c("red", "blue"))
# ----- Apprentissage d'un SVM linÃ©aire (pour essayer quand meme)
# On essaye C = 1000 pour dÃ©buter
### TODO: Apprendre un modÃ¨le lineaire et calculer erreurs apprentissage/validation
svm_model <- svm(y~.,data = dataset, kernel='linear',cost=1000,type='C-classification', scale = F)
summary(svm_model)
dessiner_marge(dataset, svm_model, 0,20,0,20.5, c("red", "blue"))
# ----- Apprentissage d'un SVM linÃ©aire (pour essayer quand meme)
# On essaye C = 1000 pour dÃ©buter
### TODO: Apprendre un modÃ¨le lineaire et calculer erreurs apprentissage/validation
svm_model <- svm(y~.,data = dataset, kernel='linear',cost=1000,type='C-classification', scale = F)
summary(svm_model)
dessiner_marge(dataset, svm_model, 0,20,0,20.5, c("red", "blue"))
# Affichage de la frontiÃ¨re avec la fonction dessiner_frontiere_svm
dessiner_frontiere_svm(train[,1:2], train$y, valid[,1:2], valid$y, svm_model, -3,4,-5,9, c("red", "blue"))
# ----- Apprentissage d'un SVM linÃ©aire (pour essayer quand meme)
# On essaye C = 1000 pour dÃ©buter
### TODO: Apprendre un modÃ¨le lineaire et calculer erreurs apprentissage/validation
svm_model <- svm(y~.,data = dataset, kernel='linear',cost=1000,type='C-classification', scale = F)
summary(svm_model)
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,7]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,7]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,7]!=p[i]){
res=res+1
}
}
p = predict(mod, train)
summary(svm_model)
p = predict(svm_model, train)
res=0
for(i in 1:nrow(train)){
if(train[i,7]!=p[i]){
res=res+1
}
}
summary(svm_model)
p = predict(svm_model, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(svm_model, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
# ----- Apprentissage d'un SVM linÃ©aire (pour essayer quand meme)
# On essaye C = 1000 pour dÃ©buter
### TODO: Apprendre un modÃ¨le lineaire et calculer erreurs apprentissage/validation
svm_model <- svm(y~.,data = train, kernel='linear',cost=1000,type='C-classification', scale = F)
summary(svm_model)
p = predict(svm_model, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(svm_model, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
# Affichage de la frontiÃ¨re avec la fonction dessiner_frontiere_svm
dessiner_frontiere_svm(train[,1:2], train$y, valid[,1:2], valid$y, svm_model, -3,4,-5,9, c("red", "blue"))
### TODO: Essayer quelques autres valeurs de C et noter les erreurs d apprentissage et validation
C = c(0.001,0.01,0.1,1,10,100)
for(i in 1:length(C)){
mod <- svm(y~.,data = train, kernel='linear',cost=C[i],type='C-classification', scale = F)
print("c = ")
print( C[i])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,7]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,7]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
### TODO: Essayer quelques autres valeurs de C et noter les erreurs d apprentissage et validation
C = c(0.001,0.01,0.1,1,10,100)
for(i in 1:length(C)){
mod <- svm(y~.,data = train, kernel='linear',cost=C[i],type='C-classification', scale = F)
print("c = ")
print( C[i])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
# ----- Apprentissage d'un SVM Ã  Noyau
#  On va maintenant utiliser un noyau gaussien (avec gamma = 1 pour commencer, et cost = 1)
svm_model <- svm(y~.,data = train, kernel='radial',cost=1,type='C-classification', gamma = 1)
### TODO: dessiner la frontiÃ¨re associÃ©e Ã  ce SVM
dessiner_frontiere_svm(train[,1:2], train$y, valid[,1:2], valid$y, svm_model, -3,4,-5,9, c("red", "blue"))
### TODO: Faire varier le gammma entre 0.01 et 2 et calculer les erreurs
gamma = c(0.01,0.1,0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8,2)
for(idxGamma in 1:length(gamma)){
svm_model <- svm(y~.,data = train, kernel='radial',cost=1,type='C-classification', gamma = gama[idxGamma])
}
for(idxGamma in 1:length(gamma)){
svm_model <- svm(y~.,data = train, kernel='radial',cost=1,type='C-classification', gamma = gamma[idxGamma])
}
for(idxGamma in 1:length(gamma)){
svm_model <- svm(y~.,data = train, kernel='radial',cost=1,type='C-classification', gamma = gamma[idxGamma])
}
### TODO: Faire varier le gammma entre 0.01 et 2 et calculer les erreurs
gamma = c(0.01,0.1,0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8,2)
for(idxGamma in 1:length(gamma)){
mod <- svm(y~.,data = train, kernel='radial',cost=1,type='C-classification', gamma = gamma[idxGamma])
print("gamma = ")
print( gamma[idxGamma])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
C = c(0.001,0.01,0.1,1,10,100)
for(i in 1:length(C)){
for(idxGamma in 1:length(gamma)){
mod <- svm(y~.,data = train, kernel='radial',cost=C[i],type='C-classification', gamma = gamma[idxGamma])
print("gamma = ")
print( gamma[idxGamma])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
}
for(i in 1:length(C)){
for(idxGamma in 1:length(gamma)){
mod <- svm(y~.,data = train, kernel='radial',cost=C[i],type='C-classification', gamma = gamma[idxGamma])
print("C = ")
print( C[i])
print("gamma = ")
print( gamma[idxGamma])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
}
for(i in 1:length(C)){
for(idxGamma in 1:length(gamma)){
mod <- svm(y~.,data = train, kernel='radial',cost=C[i],type='C-classification', gamma = gamma[idxGamma])
print("C = ")
print( C[i])
print("gamma = ")
print( gamma[idxGamma])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
}
### TODO: Faire varier le gammma entre 0.01 et 2 et calculer les erreurs
gamma = c(0.01,0.1,0.2,0.4,0.6,0.8,1,1.2,1.4,1.6,1.8,2)
C = c(0.001,0.01,0.1,1,10,100)
for(idxC in 1:length(C)){
for(idxGamma in 1:length(gamma)){
mod <- svm(y~.,data = train, kernel='radial',cost=C[idxC],type='C-classification', gamma = gamma[idxGamma])
print("C = ")
print( C[idxC])
print("gamma = ")
print( gamma[idxGamma])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
}
gamma = c(2,2.5,3,3.5,4,4.5,5)
C = c(0.001,0.01,0.1,1,10,100)
for(idxC in 1:length(C)){
for(idxGamma in 1:length(gamma)){
mod <- svm(y~.,data = train, kernel='radial',cost=C[idxC],type='C-classification', gamma = gamma[idxGamma])
print("C = ")
print( C[idxC])
print("gamma = ")
print( gamma[idxGamma])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
}
gamma = c(2,2.5,3,3.5,4,4.5,5)
C = c(0.1,1,10,100)
for(idxC in 1:length(C)){
for(idxGamma in 1:length(gamma)){
mod <- svm(y~.,data = train, kernel='radial',cost=C[idxC],type='C-classification', gamma = gamma[idxGamma])
print("C = ")
print( C[idxC])
print("gamma = ")
print( gamma[idxGamma])
p = predict(mod, train)
res=0
for(i in 1:nrow(train)){
if(train[i,3]!=p[i]){
res=res+1
}
}
print("erreur apprentissage: ")
print(res/nrow(train))
p = predict(mod, valid)
res=0
for(i in 1:nrow(valid)){
if(valid[i,3]!=p[i]){
res=res+1
}
}
print("erreur validation: ")
print(res/nrow(valid))
print("~~~~~~~~~~~~")
}
}
